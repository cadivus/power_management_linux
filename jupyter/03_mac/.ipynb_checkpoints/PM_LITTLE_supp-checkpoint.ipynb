{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y84NDSjl4C-C",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Daten Laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ByjA-x4quQZb",
    "outputId": "2b9e4985-9228-4bb0-ea17-7a8e1b3c4dee"
   },
   "outputs": [],
   "source": [
    "file_path = \"LITTLE_2025-05-10__03_52_57-normalized-supplemented.csv\"\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "id": "bHDc4Gy0v1NW",
    "outputId": "d2101a11-b6e8-42c4-9d88-7a00fdf2c9d1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop command column\n",
    "df = df.drop(columns=['command'])\n",
    "\n",
    "# Display the first few rows to understand the structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rr2KHuKu4Fnx",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Korrelationsmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "c27OgSkpw0QC",
    "outputId": "9a3180dd-1ac0-4de8-a861-1fb0132636e2"
   },
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "corr.style.background_gradient(cmap='RdYlGn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVJDoEkM30zs"
   },
   "source": [
    "## 1. Versuch (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kw2OaeRIx4f_",
    "outputId": "524f3c67-a7ce-45c5-a3b7-b145d82d38ca"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Zielvariable: Annahme, dass die erste Spalte die zu schätzende Zahl enthält\n",
    "y = df.iloc[:, 0]\n",
    "\n",
    "# Feature, das immer genutzt werden soll: 'instructions'\n",
    "X_instructions = df[['instructions:u']]\n",
    "\n",
    "# Potenzielle zusätzliche Features: Alle Spalten außer der Zielvariable und 'instructions'\n",
    "potential_features = df.drop(columns=[df.columns[0], 'instructions:u'])\n",
    "\n",
    "# Automatische Auswahl der 4 besten Features basierend auf f_regression\n",
    "selector = SelectKBest(score_func=f_regression, k=4)\n",
    "X_selected = selector.fit_transform(potential_features, y)\n",
    "\n",
    "# Ausgabe der Namen der ausgewählten Features\n",
    "selected_feature_names = potential_features.columns[selector.get_support()]\n",
    "print(\"Ausgewählte Features:\", selected_feature_names.tolist())\n",
    "\n",
    "# Falls gewünscht: Umwandeln in DataFrame, um die Spaltennamen beizubehalten\n",
    "X_selected_df = pd.DataFrame(X_selected, columns=selected_feature_names, index=df.index)\n",
    "\n",
    "# Kombination der 'instructions'-Spalte mit den ausgewählten Features\n",
    "X_combined = pd.concat([X_instructions, X_selected_df], axis=1)\n",
    "\n",
    "# Aufteilen in Trainings- und Testdaten\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training eines Regressionsmodells (z. B. Lineare Regression)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen und Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a84PPFTN38jC"
   },
   "source": [
    "## 2. Versuch (Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nfo6KJD04XNa"
   },
   "source": [
    "### Modell trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rNZQD4pwzPrD",
    "outputId": "9dbfd9cb-e9a7-4da5-9b10-47e54cd20c55"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Zielvariable: erste Spalte (angenommen, es handelt sich um numerische Werte)\n",
    "y = df.iloc[:, 0]\n",
    "\n",
    "# Fixes Feature: 'instructions'\n",
    "X_instructions = df[['instructions:u']]\n",
    "\n",
    "# Kandidaten für die automatische Feature-Auswahl:\n",
    "# Entfernen der Zielvariable und der Spalte 'instructions'\n",
    "X_candidates = df.drop(columns=[df.columns[0], 'instructions:u'])\n",
    "\n",
    "# Basisregressor für die Feature-Selektion (hier: Lineare Regression)\n",
    "model = LinearRegression()\n",
    "\n",
    "# Sequential Feature Selector (forward selection) wählt 4 zusätzliche Features aus\n",
    "sfs = SequentialFeatureSelector(\n",
    "    model,\n",
    "    n_features_to_select=4,\n",
    "    direction='forward',\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5  # 5-fache Kreuzvalidierung\n",
    ")\n",
    "\n",
    "# Durchführung der Feature-Selektion auf den Kandidatenfeatures\n",
    "sfs.fit(X_candidates, y)\n",
    "\n",
    "# Ermitteln der Namen der ausgewählten Features\n",
    "selected_feature_names = X_candidates.columns[sfs.get_support()]\n",
    "print(\"Ausgewählte Features:\", selected_feature_names.tolist())\n",
    "\n",
    "# Zusammenführen des fixen Features 'instructions' mit den ausgewählten Features\n",
    "X_selected = pd.concat([X_instructions, X_candidates[selected_feature_names]], axis=1)\n",
    "\n",
    "# Aufteilen in Trainings- und Testdaten\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modelltraining auf den kombinierten Features\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen und Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Ausgabe der Regressionsparameter\n",
    "print(\"\\nRegression Parameter:\")\n",
    "print(\"Achsenabschnitt (Intercept):\", model.intercept_)\n",
    "\n",
    "# Ausgabe der Koeffizienten mit den zugehörigen Feature-Namen\n",
    "print(\"Koeffizienten:\")\n",
    "for feature, coef in zip(X_selected.columns, model.coef_):\n",
    "    print(f\"{feature}: {coef}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAj34dML4QHy"
   },
   "source": [
    "### Modell testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j7mBZsdQ1WXw",
    "outputId": "cc14c014-6d09-41a3-eb7a-878db27ec096"
   },
   "outputs": [],
   "source": [
    "def do_regression(instructions, L1_dcache_loads, L1_icache_loads, topdown_retiring, uncore_clock_clockticks):\n",
    "  return 5.671202142645853 + instructions * -4.037434477408138e-11 + L1_dcache_loads * 3.7727498541646285e-10 + topdown_retiring * 9.090283761268329e-11 + uncore_clock_clockticks * 1.4513036090171807e-09\n",
    "\n",
    "# sleep 10 (expected 2.14)\n",
    "print(\"sleep 10 (expected 2.14)\")\n",
    "print(do_regression(165556.153198144, 44181.0547038958, 54556.2364921857, 208105.052597547, 38000929.1425307))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# sum_up_benchmark (expected 10.46)\n",
    "print(\"sum_up_benchmark (expected 10.46)\")\n",
    "print(do_regression(7658080659.63333, 1628055678.50196, 1528910300.65665, 7961570136.8392, 2696560318.9496))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# stress-ng --cpu 2 --cpu-method all --timeout 10s (expected 11.055)\n",
    "print(\"stress-ng --cpu 2 --cpu-method all --timeout 10s (expected 11.055)\")\n",
    "print(do_regression(9439119304.88239, 1242411937.11209, 1632001751.87898, 9934301707.13731, 2690003038.87471))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Versuch (Schwierig auf Colab...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class CustomSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer, der die Spalte 'instructions' immer mit einbezieht\n",
    "    und zusätzlich eine Liste von weiteren Features auswählt, die als Parameter übergeben wird.\n",
    "    \"\"\"\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Es werden immer 'instructions' plus die angegebenen features zurückgegeben\n",
    "        return X[['instructions:u'] + list(self.features)]\n",
    "\n",
    "def optimal_feature_selection_regression_cv(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Verwendet GridSearchCV, um die optimale Feature-Kombination zu finden.\n",
    "    'instructions' wird stets genutzt und zusätzlich werden 4 weitere Features\n",
    "    aus den Kandidaten (alle außer der Zielvariable und 'instructions') ausgewählt.\n",
    "\n",
    "    Rückgabe:\n",
    "      dict: Mit den besten zusätzlichen Features, dem CV-Score,\n",
    "            Test-MSE, Achsenabschnitt und Koeffizienten.\n",
    "    \"\"\"\n",
    "    # Zielvariable: erste Spalte\n",
    "    y = df.iloc[:, 0]\n",
    "    # Prädiktoren: alle Spalten außer der Zielvariable\n",
    "    X = df.drop(columns=[df.columns[0]])\n",
    "\n",
    "    # Kandidaten für zusätzliche Features: alle außer 'instructions'\n",
    "    candidates = list(X.drop(columns=['instructions:u']).columns)\n",
    "\n",
    "    # Pipeline: CustomSelector + LinearRegression\n",
    "    pipeline = Pipeline([\n",
    "        ('selector', CustomSelector(features=[])),\n",
    "        ('regressor', LinearRegression())\n",
    "    ])\n",
    "\n",
    "    # Erzeuge den Parametergrid: alle Kombinationen von 4 Features aus den Kandidaten\n",
    "    param_grid = {\n",
    "        'selector__features': [list(comb) for comb in combinations(candidates, 4)]\n",
    "    }\n",
    "\n",
    "    grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid.fit(X, y)\n",
    "\n",
    "    best_features = grid.best_params_['selector__features']\n",
    "    best_score = grid.best_score_\n",
    "\n",
    "    print(\"Beste zusätzliche Features:\", best_features)\n",
    "    print(\"Bester CV Score (neg_mean_squared_error):\", best_score)\n",
    "\n",
    "    # Aufteilen in Trainings- und Testdaten\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Bestes Modell trainieren\n",
    "    best_model = grid.best_estimator_\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(\"Finales Test-MSE:\", mse)\n",
    "\n",
    "    # Regressionsparameter ausgeben\n",
    "    regressor = best_model.named_steps['regressor']\n",
    "    print(\"\\nRegression Parameter:\")\n",
    "    print(\"Achsenabschnitt (Intercept):\", regressor.intercept_)\n",
    "\n",
    "    coef_dict = {}\n",
    "    # Die verwendeten Spalten: 'instructions:u' plus die best_features\n",
    "    features_used = ['instructions:u'] + best_features\n",
    "    print(\"Koeffizienten:\")\n",
    "    for feature, coef in zip(features_used, regressor.coef_):\n",
    "        print(f\"{feature}: {coef}\")\n",
    "        coef_dict[feature] = coef\n",
    "\n",
    "    return {\n",
    "        \"best_features\": best_features,\n",
    "        \"cv_score\": best_score,\n",
    "        \"test_mse\": mse,\n",
    "        \"intercept\": regressor.intercept_,\n",
    "        \"coefficients\": coef_dict\n",
    "    }\n",
    "\n",
    "# Beispiel zur Nutzung:\n",
    "# df = pd.read_csv('data.csv')\n",
    "# result = optimal_feature_selection_regression_cv(df)\n",
    "# print(result)\n",
    "\n",
    "optimal_feature_selection_regression_cv(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_regression(instructions, L1_dcache_stores, dTLB_loads, dTLB_stores, uncore_clock_clockticks):\n",
    "  return 20.835376283133087 + instructions * 7.719055169945736e-11 + L1_dcache_stores * 6.750946704125803e-09 + dTLB_loads * 5.752769284893428e-10 + dTLB_stores * -6.910438337198133e-09 + uncore_clock_clockticks * -4.169191670737849e-09\n",
    "\n",
    "# sleep 10 (expected 2.14)\n",
    "print(\"sleep 10 (expected 2.14)\")\n",
    "print(do_regression(165556.153198144, 26701.7192578545, 44249.7939094154, 26902.9815858347, 38000929.1425307))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# sum_up_benchmark (expected 10.46)\n",
    "print(\"sum_up_benchmark (expected 10.46)\")\n",
    "print(do_regression(7658080659.63333, 698890004.553369, 1626382945.67966, 698381494.410173, 2696560318.9496))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# stress-ng --cpu 2 --cpu-method all --timeout 10s (expected 11.055)\n",
    "print(\"stress-ng --cpu 2 --cpu-method all --timeout 10s (expected 11.055)\")\n",
    "print(do_regression(9439119304.88239, 561931144.341175, 1237133407.39931, 559806745.508526, 2690003038.87471))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
