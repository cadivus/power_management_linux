{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y84NDSjl4C-C"
   },
   "source": [
    "## Daten Laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ByjA-x4quQZb",
    "outputId": "c8af25e8-c8be-4c1c-c986-bfb370274219"
   },
   "outputs": [],
   "source": [
    "file_path = \"BIG_2025-05-09__22_03_20-normalized-supplemented.csv\"\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "id": "bHDc4Gy0v1NW",
    "outputId": "a3c0a595-b522-40e0-dd15-8b65ccc5f9cd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Drop command column\n",
    "df = df.drop(columns=['command'])\n",
    "\n",
    "# Display the first few rows to understand the structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rr2KHuKu4Fnx"
   },
   "source": [
    "## Korrelationsmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "c27OgSkpw0QC",
    "outputId": "1b440248-d95a-4dac-91f4-2d7fb5b0ed89"
   },
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "corr.style.background_gradient(cmap='RdYlGn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVJDoEkM30zs"
   },
   "source": [
    "## 1. Versuch (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kw2OaeRIx4f_",
    "outputId": "40807f8b-9e23-40f8-fa12-36f92c3227da"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Zielvariable: Annahme, dass die erste Spalte die zu schätzende Zahl enthält\n",
    "y = df.iloc[:, 0]\n",
    "\n",
    "# Feature, das immer genutzt werden soll: 'instructions'\n",
    "X_instructions = df[['instructions:u']]\n",
    "\n",
    "# Potenzielle zusätzliche Features: Alle Spalten außer der Zielvariable und 'instructions'\n",
    "potential_features = df.drop(columns=[df.columns[0], 'instructions:u'])\n",
    "\n",
    "# Automatische Auswahl der 4 besten Features basierend auf f_regression\n",
    "selector = SelectKBest(score_func=f_regression, k=4)\n",
    "X_selected = selector.fit_transform(potential_features, y)\n",
    "\n",
    "# Ausgabe der Namen der ausgewählten Features\n",
    "selected_feature_names = potential_features.columns[selector.get_support()]\n",
    "print(\"Ausgewählte Features:\", selected_feature_names.tolist())\n",
    "\n",
    "# Falls gewünscht: Umwandeln in DataFrame, um die Spaltennamen beizubehalten\n",
    "X_selected_df = pd.DataFrame(X_selected, columns=selected_feature_names, index=df.index)\n",
    "\n",
    "# Kombination der 'instructions'-Spalte mit den ausgewählten Features\n",
    "X_combined = pd.concat([X_instructions, X_selected_df], axis=1)\n",
    "\n",
    "# Aufteilen in Trainings- und Testdaten\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Training eines Regressionsmodells (z. B. Lineare Regression)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen und Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a84PPFTN38jC"
   },
   "source": [
    "## 2. Versuch (Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nfo6KJD04XNa"
   },
   "source": [
    "### Modell trainieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rNZQD4pwzPrD",
    "outputId": "df09b71f-9dd8-4140-fbaa-5220b0abd157"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Zielvariable: erste Spalte (angenommen, es handelt sich um numerische Werte)\n",
    "y = df.iloc[:, 0]\n",
    "\n",
    "# Fixes Feature: 'instructions'\n",
    "X_instructions = df[['instructions:u']]\n",
    "\n",
    "# Kandidaten für die automatische Feature-Auswahl:\n",
    "# Entfernen der Zielvariable und der Spalte 'instructions'\n",
    "X_candidates = df.drop(columns=[df.columns[0], 'instructions:u'])\n",
    "\n",
    "# Basisregressor für die Feature-Selektion (hier: Lineare Regression)\n",
    "model = LinearRegression()\n",
    "\n",
    "# Sequential Feature Selector (forward selection) wählt 4 zusätzliche Features aus\n",
    "sfs = SequentialFeatureSelector(\n",
    "    model,\n",
    "    n_features_to_select=4,\n",
    "    direction='forward',\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5  # 5-fache Kreuzvalidierung\n",
    ")\n",
    "\n",
    "# Durchführung der Feature-Selektion auf den Kandidatenfeatures\n",
    "sfs.fit(X_candidates, y)\n",
    "\n",
    "# Ermitteln der Namen der ausgewählten Features\n",
    "selected_feature_names = X_candidates.columns[sfs.get_support()]\n",
    "print(\"Ausgewählte Features:\", selected_feature_names.tolist())\n",
    "\n",
    "# Zusammenführen des fixen Features 'instructions' mit den ausgewählten Features\n",
    "X_selected = pd.concat([X_instructions, X_candidates[selected_feature_names]], axis=1)\n",
    "\n",
    "# Aufteilen in Trainings- und Testdaten\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modelltraining auf den kombinierten Features\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Vorhersagen und Evaluation\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Ausgabe der Regressionsparameter\n",
    "print(\"\\nRegression Parameter:\")\n",
    "print(\"Achsenabschnitt (Intercept):\", model.intercept_)\n",
    "\n",
    "# Ausgabe der Koeffizienten mit den zugehörigen Feature-Namen\n",
    "print(\"Koeffizienten:\")\n",
    "for feature, coef in zip(X_selected.columns, model.coef_):\n",
    "    print(f\"{feature}: {coef}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NAj34dML4QHy"
   },
   "source": [
    "### Modell testen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j7mBZsdQ1WXw",
    "outputId": "3ff1f32b-5e21-4923-b70a-912f2539fd14"
   },
   "outputs": [],
   "source": [
    "def do_regression(instructions, dTLB_loads, cache_misses, cstate_core_c6_residency, unc_m_clockticks):\n",
    "  return -417.5947581136508 + instructions * 4.65216312028892e-11 + dTLB_loads * 7.883112920541098e-10 + cache_misses * -1.9612014211040394e-07 + cstate_core_c6_residency * 4.18166881063541e-09 + unc_m_clockticks * 1.3126082256029556e-07\n",
    "\n",
    "# sleep 10 (expected 2.125)\n",
    "print(\"sleep 10 (expected 2.125)\")\n",
    "print(do_regression(168777.863665457, 41575.009771487, 1361.36887244182, 27904585074.6458, 177728131.709414))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# sum_up_benchmark (expected 14.505)\n",
    "print(\"sum_up_benchmark (expected 14.505)\")\n",
    "print(do_regression(11664606808.4586, 2175275696.69778, 6728517.43140718, 28062502860.874, 2390791162.75018))\n",
    "\n",
    "print(\"\")\n",
    "\n",
    "# stress-ng --cpu 2 --cpu-method all --timeout 10s (expected 16.39)\n",
    "print(\"stress-ng --cpu 2 --cpu-method all --timeout 10s (expected 16.39)\")\n",
    "print(do_regression(15248513772.5695, 2013098946.16777, 56550.6387563183, 27893525904.2613, 2389444709.36587))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4u8ru_eGBZZu"
   },
   "source": [
    "## 3. Versuch (Schwierig auf Colab...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fEK9PRIRBb0W",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class CustomSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transformer, der die Spalte 'instructions' immer mit einbezieht\n",
    "    und zusätzlich eine Liste von weiteren Features auswählt, die als Parameter übergeben wird.\n",
    "    \"\"\"\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Es werden immer 'instructions' plus die angegebenen features zurückgegeben\n",
    "        return X[['instructions:u'] + list(self.features)]\n",
    "\n",
    "def optimal_feature_selection_regression_cv(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Verwendet GridSearchCV, um die optimale Feature-Kombination zu finden.\n",
    "    'instructions' wird stets genutzt und zusätzlich werden 4 weitere Features\n",
    "    aus den Kandidaten (alle außer der Zielvariable und 'instructions') ausgewählt.\n",
    "\n",
    "    Rückgabe:\n",
    "      dict: Mit den besten zusätzlichen Features, dem CV-Score,\n",
    "            Test-MSE, Achsenabschnitt und Koeffizienten.\n",
    "    \"\"\"\n",
    "    # Zielvariable: erste Spalte\n",
    "    y = df.iloc[:, 0]\n",
    "    # Prädiktoren: alle Spalten außer der Zielvariable\n",
    "    X = df.drop(columns=[df.columns[0]])\n",
    "\n",
    "    # Kandidaten für zusätzliche Features: alle außer 'instructions'\n",
    "    candidates = list(X.drop(columns=['instructions:u']).columns)\n",
    "\n",
    "    # Pipeline: CustomSelector + LinearRegression\n",
    "    pipeline = Pipeline([\n",
    "        ('selector', CustomSelector(features=[])),\n",
    "        ('regressor', LinearRegression())\n",
    "    ])\n",
    "\n",
    "    # Erzeuge den Parametergrid: alle Kombinationen von 4 Features aus den Kandidaten\n",
    "    param_grid = {\n",
    "        'selector__features': [list(comb) for comb in combinations(candidates, 4)]\n",
    "    }\n",
    "\n",
    "    grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid.fit(X, y)\n",
    "\n",
    "    best_features = grid.best_params_['selector__features']\n",
    "    best_score = grid.best_score_\n",
    "\n",
    "    print(\"Beste zusätzliche Features:\", best_features)\n",
    "    print(\"Bester CV Score (neg_mean_squared_error):\", best_score)\n",
    "\n",
    "    # Aufteilen in Trainings- und Testdaten\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Bestes Modell trainieren\n",
    "    best_model = grid.best_estimator_\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(\"Finales Test-MSE:\", mse)\n",
    "\n",
    "    # Regressionsparameter ausgeben\n",
    "    regressor = best_model.named_steps['regressor']\n",
    "    print(\"\\nRegression Parameter:\")\n",
    "    print(\"Achsenabschnitt (Intercept):\", regressor.intercept_)\n",
    "\n",
    "    coef_dict = {}\n",
    "    # Die verwendeten Spalten: 'instructions' plus die best_features\n",
    "    features_used = ['instructions:u'] + best_features\n",
    "    print(\"Koeffizienten:\")\n",
    "    for feature, coef in zip(features_used, regressor.coef_):\n",
    "        print(f\"{feature}: {coef}\")\n",
    "        coef_dict[feature] = coef\n",
    "\n",
    "    return {\n",
    "        \"best_features\": best_features,\n",
    "        \"cv_score\": best_score,\n",
    "        \"test_mse\": mse,\n",
    "        \"intercept\": regressor.intercept_,\n",
    "        \"coefficients\": coef_dict\n",
    "    }\n",
    "\n",
    "# Beispiel zur Nutzung:\n",
    "# df = pd.read_csv('data.csv')\n",
    "# result = optimal_feature_selection_regression_cv(df)\n",
    "# print(result)\n",
    "\n",
    "optimal_feature_selection_regression_cv(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
